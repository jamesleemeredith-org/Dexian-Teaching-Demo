{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Analytics\n",
    "\n",
    "## Ingesting Data with Pandas\n",
    "\n",
    "![Python and Pandas!](images/PythonPandasandDataIngestion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OBJECTIVES: \n",
    "\n",
    "- What is Pandas\n",
    "- Pandas & NumPy\n",
    "- Pandas and Jupyter Notebooks\n",
    "- What Pandas can do\n",
    "- Reading Data from: \n",
    "    - CSV files\n",
    "    - Excel files\n",
    "    - SQL databases\n",
    "- Hands-on Data Wrangling\n",
    "- In-Class Group Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Pandas\n",
    "\n",
    "- **Pandas** - An open-source Python package that is widely used\n",
    "- Built on top of NumPy (supports 1+ D arrays)\n",
    "- **Stands for either**: \n",
    "    1. Panel Data \n",
    "    2. Python Data Analysis\n",
    "- Created by Wes McKinney in 2008\n",
    "- **NOTE**: In curriculum are two additional links on Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES\n",
    ">\n",
    "> ## What is Pandas\n",
    "> \n",
    "> - An open-source Python package that is most widely used for data science/data analysis and machine learning tasks. \n",
    "> - Built on top of NumPy which provides support for multi-dimensional arrays.\n",
    "> - References both “Panel Data” and “Python Data Analysis”\n",
    "> - The name Pandas is derived from the word \"Panel Data\"\n",
    "> - Created by Wes McKinney in 2008\n",
    "> - Official documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html#user-guide\n",
    "> - Community tutorials: https://pandas.pydata.org/pandas-docs/stable/getting_started/tutorials.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas & NumPy - These two libraries are the best within data science\n",
    "| **Pandas** | **NumPy** |\n",
    "| ---- | ---- |\n",
    "| A high-level data manipulation tool built on NumPy | Supports large 1+ D arrays and high-level mathematical functions |\n",
    "| **Dataframe (df)** - Structured like a table or spreadsheet (rows and columns). Uses some NumPy functions. | |\n",
    "| Uses Series | Uses ndarray's |\n",
    "| Greater memory and slower | Less memory and faster |\n",
    "| Mainly works with tabular data | Works with numerical data |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES\n",
    "> \n",
    "> ## Pandas & NumPy\n",
    "> \n",
    "> - NumPy is a library that adds support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\n",
    "> - Pandas is a high-level data manipulation tool that is built on the NumPy package\n",
    "> - Pandas offers an in-memory 2d table object called a DataFrame\n",
    "> - A DataFrame is structured like a table or spreadsheet -- with rows and columns\n",
    "> - There are a few functions that exist in NumPy that we use specifically on Pandas DataFrames\n",
    "> - Just as the \"ndarray\" is the foundation of NumPy, the \"Series\" is the core object of Pandas\n",
    "> - NumPy consumes less memory than Pandas, and is faster than Pandas\n",
    "> - These two libraries are the best libraries for data science applications\n",
    "> - Pandas mainly works with tabular data, whereas NumPy works with numerical data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas & Jupyter Notebooks\n",
    "- Benefits to using Pandas within Jupyter Notebooks:\n",
    "    - A good environment for data exploration and modeling \n",
    "    - Ability to execute code in a particular cell, opposed to one large file (saves time)\n",
    "    - Can easily visualize dataframes and plots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES\n",
    "> \n",
    "> ## Pandas & Jupyter Notebooks\n",
    "> \n",
    "> Jupyter Notebooks offer a good environment for using pandas to do data exploration and modeling, but pandas can also be used in text editors just as easily.\n",
    "> \n",
    "> Jupyter Notebooks give us the ability to execute code in a particular cell as opposed to running the entire file. This saves a lot of time when working with large datasets and complex transformations. \n",
    "> \n",
    "> Notebooks also provide an easy way to visualize pandas’ DataFrames and plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can Pandas do?\n",
    "- **Perform 5 data analysis steps**:\n",
    "    1. load\n",
    "    2. manipulate\n",
    "    3. prepare\n",
    "    4. model\n",
    "    5. analyze\n",
    "- It takes data files (e.g., CSV, TSV, SQL) and creates a dataframe (with rows and columns)\n",
    "- World-leading Data Scientists ranked it *The Best Python Data Analysis and Manipulation Tool*\n",
    "- **Pandas can do**:\n",
    "\n",
    "|    |    |\n",
    "|----|----|\n",
    "| Data Cleansing | Data fill |\n",
    "| Data normalization | Merges and joins |\n",
    "| Data visualization | Statistical analysis |\n",
    "| Data inspection | Loading and saving data |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES\n",
    "\n",
    "## What can Pandas do?\n",
    "\n",
    "Pandas can perform five significant steps required for processing and analysis of data, irrespective of the origin of the data, -- load, manipulate, prepare, model, and analyze.\n",
    "\n",
    "What’s cool about Pandas is that it takes data (like a CSV or TSV file, or a SQL database) and creates a Python object with rows and columns called a 'data frame' that looks very similar to table representation in statistical software (think Excel).\n",
    "\n",
    "In fact, with Pandas, you can do everything that makes world-leading data scientists vote Pandas as the best Python data analysis and manipulation tool available.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Using Pandas\n",
    "- Must install Pandas, and NumPy is required:\n",
    "    - **Windows**: `pip install pandas`\n",
    "    - **Mac**: `pip3 install pandas` or `python3 install pandas`\n",
    "- After installed, must import each time you use the library:\n",
    "    - **Syntax Example**: `import pandas as pd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading data from `CSV files` into a `DataFrame`:\n",
    "- `pd.read_csv()` - Retrieves CSV file data to a dataframe\n",
    "- Data is usually separated by commas (default). \n",
    "    - Other separators include: \n",
    "        - semi-colon (';')\n",
    "        - colon (':')\n",
    "        - vertical bar ('|')\n",
    "        - tab ('\\t')\n",
    "    - To change separator, use `sep='<delimiter>'`\n",
    "        - Example: `df = pd.read_csv(file_path, sep='|')`\n",
    "- Can open in Notepad but format will be off. Better to use VS Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### NOTES\n",
    ">\n",
    "> ### Reading data from `CSV files` into a `DataFrame`:\n",
    "> Read all about the [Syntax and use of `.read_csv()`.](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)\n",
    ">\n",
    "> A simple way to store big data sets is to use CSV files (comma separated files).\n",
    "> \n",
    "> CSV files contains plain text and is a well know format that can be read by everyone including Pandas.\n",
    "> \n",
    "> You can open it in Notepad but the format will be off; Use VS Code instead.\n",
    "> \n",
    "> To access data from the CSV file, we require a function `.read_csv()` that retrieves data in the form of the DataFrame.\n",
    "> \n",
    "> By default, a `CSV` is separated by commas. But one can use other separators as well. \n",
    "> \n",
    "> The `pandas.read_csv()` function is not limited to reading the CSV file with default separator (i.e. comma). It can be used for other separators such as `;` or `|` or `:`. \n",
    "> \n",
    "> To load CSV files with such separators, the `sep=` parameter is used to pass the separator used in the CSV file. Example: --\n",
    "> ```python\n",
    ">     f = pd.read_csv(\"datafile2.csv\", sep='|')\n",
    "> ```\n",
    "> \n",
    "> For our example, we'll use a file from the resources folder in the curriculum. The filepath to the CSV file is `./resources/GREENCOMPUTERS500.csv`.\n",
    "> \n",
    "> Lets first look at the data by opening the raw CSV in VSCode -> [Top 500 Green Computers](resources/GREENCOMPUTERS500.csv)\n",
    "> \n",
    "> From this data we can see that we have a file with many columns.\n",
    "> \n",
    "> Lets see what it looks like when we import the data into a DataFrame ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# FOLLOW ALONG: importing a CSV file\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### NOTES: importing a CSV file\n",
    "# In curriculum, use file: 'GREENCOMPUTERS500.csv'\n",
    "# First view dataset in curriculum (raw file)\n",
    "# index_col -> columns to use as the row labels of the DataFrame. In this case,\n",
    "# column 0 of the CSV (Rank), will be used as the index label for our rows.\n",
    "green = pd.read_csv('./resources/GREENCOMPUTERS500.csv',index_col=0)\n",
    "green.info()\n",
    "green"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating a `.csv` file from scratch\n",
    "- When creating a .csv file, it will create a 'row index' by default\n",
    "- Can save .csv without indexes\n",
    "- Unless delimiter is specified, a comma will be the default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### NOTES\n",
    "> Read all about how to use [pandas.DataFrame.to_csv](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)\n",
    ">\n",
    "> ### Creating a `.csv` file from scratch\n",
    "> \n",
    "> We can export a Pandas DataFrame to a CSV file by using the Pandas `to_csv()` method. By default, this method exports a DataFrame to a CSV file with \"row index\" as the first column, and a comma as the delimiter. \n",
    "> \n",
    "> With the `sep:` we can specify a custom delimiter for the CSV output, instead of a comma.\n",
    "> \n",
    "> One can also save the CSV without indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#  FOLLOW ALONG: creating a .csv from scratch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### NOTES: creating a .csv from scratch\n",
    "\n",
    "# Create dataframe\n",
    "cities = pd.DataFrame([[\"St. Louis\", \"Missouri\"], [\"Atlanta\", \"Georgia\"]],\n",
    "                        columns=[\"City\", \"State\"])\n",
    "cities\n",
    "\n",
    "# write df to csv file\n",
    "cities.to_csv('cities.csv')\n",
    "\n",
    "# View newly created .csv\n",
    "df = pd.read_csv('cities.csv')\n",
    "print(df)\n",
    "\n",
    "# Saving .csv without indexes (index -> Write row names (index))\n",
    "cities.to_csv('cities.csv', index=False)\n",
    "df = pd.read_csv('cities.csv')\n",
    "print(df)\n",
    "\n",
    "# write df to new csv file with delimiter set to tab ('\\t')\n",
    "# TODO: open file to view difference\n",
    "cities.to_csv('citiesT.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pandas Data Wrangling with a CSV file\n",
    "\n",
    "Next, we will use the 'data.csv' file under the resources folder in the curriculum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### NOTES\n",
    ">\n",
    "> ## Pandas Data Wrangling with a CSV file\n",
    ">\n",
    "> We will reuse the data file we introduced in the 1st Pandas session. For our example, we will use a file from the resources folder in the curriculum. \n",
    ">\n",
    "> The filepath to the CSV file is `./resources/data.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# FOLLOW ALONG: read and print a summary of a DataFrame\n",
    "# Same print commands from Section 5.2\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./resources/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### NOTES: read and print a summary of a DF\n",
    "\n",
    "# Print first and last 5 rows (if default)\n",
    "df\n",
    "\n",
    "# Print first 10 rows\n",
    "print(df.head(10))\n",
    "\n",
    "# Print last 12 rows\n",
    "print(df.tail(12))\n",
    "\n",
    "# Print summary of number of columns, column labels, data types, memory usage, range index, and non-null values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A closer look at the DataFrame Info …\n",
    "\n",
    "![DataFrame Info Display](images/Pandas_DF_InfoDisplay.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### NOTES\n",
    ">\n",
    "> Looking at this Info reveals that there are 5 rows in the '`Calories`' column without data.\n",
    ">\n",
    "> Nulls are bad! Nulls are the wrong result when you analyze data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### **Dropping the NULLs -- `.dropna()`**\n",
    "- Pull-up: `data.csv` in the Resource folder\n",
    "    - \"Calories\" column: No values (a.k.a. NULLS) in rows: 19, 29, 93, 120, 143\n",
    "- `.dropna()` - Removes rows will NULL values   \n",
    "    - `dropna()` or `dropna(inplace=False)`: Removes NULLS in a new dataframe\n",
    "    - `dropna(inplace=True)` - Removes NULLS in original dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### NOTES\n",
    ">\n",
    "> #### **Dropping the NULLs -- `.dropna()`**\n",
    ">\n",
    "> If you studied the output you should have found that the following lines contain NULLs in the 'Calories' column: 19, 29, 93, 120, 143\n",
    ">\n",
    "> We will use the `.dropna()` method to drop the rows that contains NULL values.\n",
    ">\n",
    "> The `dropna()` method returns a new DataFrame object unless the `INPLACE` parameter is set to `True`, in that case the `dropna()` method does the removing in the original DataFrame instead.\n",
    ">\n",
    "> **Remember to work with a copy of your dataset so the original data stays safe.**\n",
    ">\n",
    "> Note: In the example below, this DOES NOT change the original DataFrame BECAUSE we are using `new_df`.\n",
    ">\n",
    "> Read more about the `.dropna()` method ... [pandas.DataFrame.dropna](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# FOLLOW ALONG: Let's find the NULLS ...\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### NOTES: Let's find the NULLS ...\n",
    "\n",
    "# View entire dataframe (Exception: There is a max limit)\n",
    "#REMINDER: Check rows 19, 29, 93, 120, 143\n",
    "print(df.to_string())\n",
    "\n",
    "# Drop NULLS to a new dataframe (dropna() returns a new DF unless \"inplace=True\" specified)\n",
    "new_df = df.dropna()\n",
    "print(new_df.to_string())\n",
    "\n",
    "#Drop NULLS in original dataframe\n",
    "df.dropna(inplace = True)\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### **Replacing Nulls -- `.fillna()`**\n",
    "- Sometimes it is best to replace NULLS, rather than drop (delete) them\n",
    "- `fillna()` - Replaces NULLS with a specified value\n",
    "    - Can use `inplace=True` in () to update original dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### NOTES\n",
    ">\n",
    "> #### **Replacing Nulls -- `.fillna()`**\n",
    "> \n",
    "> Often, for data integrity sake, we do not want to drop NULLs, but transform and preserve them. We can replace NULLS with other values, or even calculations.\n",
    "> \n",
    "> The `fillna()` method replaces the NULL values with a specified value.\n",
    "> \n",
    "> The `fillna()` method returns a new DataFrame object unless the `inplace` parameter is set to True, in that case the `fillna()` method does the replacing in the original DataFrame instead.\n",
    ">\n",
    "> Read more about the `.fillna()` method ... [pandas.DataFrame.fillna](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# FOLLOW ALONG: replace NULLS with a specified value\n",
    "# REMINDER: Check rows 19, 29, 93, 120, 143\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./resources/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### NOTES: Replace ALL NULLs with the value 130\n",
    "df.fillna(130, inplace = True)\n",
    "print(df.to_string())\n",
    "\n",
    "# Replace ALL NULLS with the value 130, ONLY in the \"Calories\" column\n",
    "df[\"Calories\"].fillna(130, inplace = True)\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### **Replace Nulls using Mean, Median, Mode**\n",
    "- Can fill in NULLS with statistical values \n",
    "- `mean()` - Average\n",
    "    - Ex: 2, 3, 4, 5 --> Mean = 3.5 \n",
    "- `median()` - Middle value \n",
    "    - Ex: 1, 2, 3, 4 --> Median = 2.5\n",
    "- `mode()` - Most frequent Value\n",
    "    - Ex: 1, 1, 2, 3, 4 --> Mode = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### NOTES\n",
    ">\n",
    "> #### **Replace Nulls using Mean, Median, Mode**\n",
    "> \n",
    "> We can replace NULLS with specific calculations to \"fill in\" missing data with valid similar data.\n",
    "> \n",
    "> A common way to replace empty cells, is to calculate the Mean, Median or Mode value of the column.\n",
    "> \n",
    "> Pandas uses the `mean()`, `median()` and `mode()` methods to calculate the respective values for a specified column.\n",
    "> \n",
    "> - **Mean** = the average value\n",
    "> - **Median** = the value in the middle, after you have sorted all the values ascending\n",
    "> - **Mode** = the value that appears most frequently\n",
    "> \n",
    "> One of the key points is to decide which technique, out of the above-mentioned imputation techniques, to use for getting the most appropriate approximation for the missing values.\n",
    "> \n",
    ">The goal is to find out which is a better measure of the central tendency of data and use that value for replacing missing values appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# FOLLOW ALONG: replace NULLS with MEAN\n",
    "# REMINDER: Check rows 19, 29, 93, 120, 143\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./resources/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### NOTES: replace NULLS with MEAN\n",
    "\n",
    "# Calculate the Mean (x = round(x, 2))\n",
    "'''\n",
    "The round() function returns a floating point number that is a rounded version of the specified number,\n",
    " with the specified number of decimals. The default number of decimals is 0,\n",
    "  meaning that the function will return the nearest integer.\n",
    "'''\n",
    "col_mean = round((df[\"Calories\"].mean()), 2)\n",
    "# print(col_mean)\n",
    "\n",
    "# Replace NULLS with MEAN \n",
    "df[\"Calories\"].fillna(col_mean, inplace = True)\n",
    "#print(df.to_string())\n",
    "\n",
    "#-----------------------------------------------\n",
    "\n",
    "#Calculate the Median\n",
    "col_median = df[\"Calories\"].median()\n",
    "print(col_median)\n",
    "\n",
    "#Replace NULLS with Median\n",
    "df[\"Calories\"].fillna(col_median, inplace =True)\n",
    "# print(df.to_string())\n",
    "\n",
    "#-----------------------------------------------\n",
    "\n",
    "#Calculate the Mode\n",
    "# mode() returns a dataframe with the mode values (calculates mode of each column, but here we are specifying the column)\n",
    "col_mode = df[\"Calories\"].mode()[0]\n",
    "print(col_mode)\n",
    "\n",
    "# Replace NULLS with Mode\n",
    "df[\"Calories\"].fillna(col_mode, inplace = True)\n",
    "# print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### **Replacing Incorrect data in DataFrames**\n",
    "- `.replace()` - Replaces (or swaps) values with another value\n",
    "    - It can replace strings, regex, lists, dictionaries, series, numbers, etc. \n",
    "    - **Syntax Example**: `df.replace(to_replace = 'Value being replaced', value = 'Specified replaced value')`\n",
    "\n",
    "| Code | Plain Language                                                                                           |\n",
    "|----|----------------------------------------------------------------------------------------------------------|\n",
    "| df.replace(0, 5) | Replace ALL of the 0s in your DataFrame with 5s                                                          |\n",
    "| df.replace([0, 1, 2, 3], 4) | Replace ALL the 0s, 1s, 2s, 3s in your DataFrame with 4s                                                 |\n",
    "| df.replace({0: 10, 1: 100}) | Replace ALL, multiple values – Replace 0s with 10s, and 1s with 100s.                                    |\n",
    "| df.replace({'A': 0, 'B': 5}, 100) | Replace ALL within specified columns - 0’s in column “A” with 100, and replace 5s in column “B” with 100 |\n",
    "| df.replace({'C': {1: 100, 3: 300}}) | Within a specified column, replace multiple values – in “C” replace 1s with 100 and 3s with 300          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### NOTES\n",
    "> \n",
    "> #### **Replacing Incorrect data in DataFrames**\n",
    "> \n",
    "> So, you want to replace values in your DataFrame with something else? No problem. That is where Pandas Replace comes in.\n",
    "> \n",
    "> Pandas `DataFrame.replace()` is a small but powerful function that will replace (or swap) values in your DataFrame with another value. It can replace strings, regex, lists, dictionaries, series, numbers, etc. from a Dataframe. \n",
    "> \n",
    "> Every instance of the provided value is replaced after a thorough search of the full DataFrame, depending on the parameters used...\n",
    "> \n",
    ">     Syntax:     df.replace(to_replace = 'what you want to replace',\n",
    ">                             value = 'what you want to replace it with')\n",
    ">\n",
    "> For full detail on how to use all these options please refer to the documentation on [pandas.DataFrame.replace](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# FOLLOW ALONG: replacing values\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'X': [1, 2, 3, 4, 5],\n",
    "                    'Y': [5, 6, 7, 8, 9],\n",
    "                    'Z': ['z', 'y', 'x', 'w', 'v']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### NOTES: replacing values\n",
    "\n",
    "# Replace all 2s with 20s\n",
    "df1 = df.replace(to_replace=2, value=20)\n",
    "print(df1)\n",
    "\n",
    "# Replace all 1s, 3s, and 5s with 20\n",
    "df2 = df.replace(to_replace=[1,3,5], value=20)\n",
    "print(df2)\n",
    "\n",
    "# Replace 1s with 10s; 'z's with 'zz's; and 'v's with 'vvv's\n",
    "df4 = df.replace(to_replace={1: 10, 3: 30, 5:50, 'z':'zz', 'v':'vvv'})\n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### **Fixing Dates in DataFrames**\n",
    "- Use file: `./resources/data1.csv`\n",
    "- Ability to:\n",
    "    - Change a date format\n",
    "    - Correct dates based on region\n",
    "    - Flag incorrect dates\n",
    "    - Fix them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### NOTES\n",
    "> \n",
    "> #### **Fixing Dates in DataFrames**\n",
    "> \n",
    "> In most of the big data scenarios , there will be a requirement to fix date issues. It could be necessary to flip a date format, change date formats, or to correct them based on certain region, flag incorrect dates, and fix them appropriately.\n",
    "> \n",
    "> For the next examples we will use a much smaller dataset from the resources folder.\n",
    "> \n",
    "> The filepath to the CSV file is `./resources/data1.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# FOLLOW ALONG: fixing dates in DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data1.csv\")\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### NOTES: fixing dates in DataFrames\n",
    "\n",
    "# Convert \"Date\" series into a series of datetime objects\n",
    "# (notice row 12 is correctly changes and row 4 changes from NaN to NaT type)\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "# print(df.to_string())\n",
    "\n",
    "# Drop NULL dates inplace\n",
    "# Notice row 4 is now gone (NaT value for datetime)\n",
    "df.dropna(subset=['Date'], inplace = True)\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### **Fixing wrong info in a specific LOCATION**\n",
    "- To access or change a single value:\n",
    "    - `loc()` - Can be used to access a single value or group of rows\n",
    "    - `at()` - Use ONLY for setting a single value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### NOTES\n",
    ">\n",
    "> #### **Fixing wrong info in a specific LOCATION**\n",
    ">\n",
    "> Pandas provides two ways, `loc()` and `at()`, to access or change a single value of a DataFrame.\n",
    ">\n",
    "> - Use `at()` if you only need to get or set a single value in a DataFrame or Series.\n",
    "> - On the other hand `loc()`can be used to access a single value but also to access a group of rows and columns by a label or labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# FOLLOW ALONG: fixing wrong info in a specific LOCATION\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data1.csv\")\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "### NOTES: fixing wrong info in a specific LOCATION\n",
    "\n",
    "# Change line 9 \"Duration\" from 60 to be 45\n",
    "# row 9, column \"Duration\"\n",
    "df.loc[9, 'Duration'] = 45\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### **Fixing wrong info in LARGE sets by looping**\n",
    "- Wrong data can also be a typo\n",
    "    - **Example**: The value says 199, but it's supposed to be 1.99\n",
    "- Loops can help correct typos\n",
    "\n",
    "#### **Dropping / Deleting rows by looping**\n",
    "- Instead of correcting typos, they can be removed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### NOTES \n",
    ">\n",
    "> #### **Fixing wrong info in LARGE sets by looping**\n",
    ">\n",
    "> \"Wrong data\" does not have to be \"empty cells\" or \"wrong format\", it can just be wrong, like if someone registered \"199\" instead of \"1.99\".\n",
    ">\n",
    "> Loop through all values in the \"Duration\" column; If the value is higher than 120, set it to 120:\n",
    ">\n",
    ">\n",
    "> #### **Dropping / Deleting rows from a DataFrame**\n",
    ">\n",
    "> Another way of handling wrong data is to remove the rows that contains wrong data.\n",
    ">\n",
    "> This way you do not have to find out what to replace them with, and there is a good chance you do not need them to do your analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# FOLLOW ALONG: Fixing wrong info in LARGE sets by looping\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./resources/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### NOTES: Fixing wrong info in LARGE sets by looping\n",
    "\n",
    "# Loop through all values in the \"Duration\" column; If the value is >120, set it to 120\n",
    "duration_series = df[\"Duration\"]\n",
    "print(duration_series)\n",
    "\n",
    "for index, duration in enumerate(duration_series):\n",
    "    if duration > 120:\n",
    "        df.loc[index, \"Duration\"] = 120\n",
    "\n",
    "# example value to showcase the change\n",
    "print(df.loc[60])\n",
    "\n",
    "# #-----------------------------------------------------\n",
    "# DROP items (COMMENT OUT PREVIOUS SECTION & RELOAD DF\n",
    "print(df.info())\n",
    "for x in df.index:\n",
    "    if df.loc[x, \"Duration\"] > 120:\n",
    "        df.drop(x, inplace = True)\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### **Discovering Duplicates**\n",
    "- `duplicated()` - Identifies duplicated rows\n",
    "    - Returns a Boolean value for each row\n",
    "        - True = A duplicated row\n",
    "        - False = Not a duplicated row\n",
    "\n",
    "#### **Removing Duplicates**\n",
    "- `drop_duplicates()` - Removed duplicated rows\n",
    "    - Reminder: `inplace = True` Removes duplicates from original dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### NOTES\n",
    ">\n",
    "> #### **Discovering Duplicates**\n",
    ">\n",
    "> Duplicate rows are rows that have been registered more than one time.\n",
    "> To discover duplicates, we can use the `duplicated()` method.\n",
    ">\n",
    "> The `duplicated()` method returns a Boolean values for each row; in other words, it returns `True` for every row that is a duplicate, otherwise `False`.\n",
    ">\n",
    ">\n",
    "> #### **Removing Duplicates**\n",
    "> \n",
    "> To remove duplicates, use the `drop_duplicates()` method.\n",
    "> \n",
    "> Remember: The `inplace = True` will make sure that the method does NOT return a new DataFrame, but it will remove all duplicates from the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# FOLLOW ALONG: duplicates\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### NOTES: duplicates\n",
    "\n",
    "# Finding all duplicates\n",
    "print(df.duplicated())\n",
    "\n",
    "# Remove all Duplicates (row 12)\n",
    "df.drop_duplicates(inplace = True)\n",
    "print(df.duplicated())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More practice with CSV files\n",
    "- CSV file we will use in the resources folder: ('resources/titanic.csv')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### NOTES\n",
    ">\n",
    "> ### More practice with CSV files - Titanic\n",
    "> \n",
    "> First, we need to gather our data.\n",
    "> \n",
    "> We can either use the data from our resources directory, or we can import our data from the WEB.\n",
    "> \n",
    "> The filepath to the CSV file in the curriculum resources folder is [\"./resources/titanic.csv\"](resources/titanic.csv).\n",
    "> \n",
    "> Else, the URL to the data file on the WEB is ... https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\n",
    "> \n",
    "> You can download that file to your machine, or we will pull that file directly in our code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# FOLLOW ALONG: titanic\n",
    "import pandas as pd\n",
    "\n",
    "# Pull data from a file:\n",
    "titanic_data  = pd.read_csv(\"./resources/titanic.csv\")\n",
    "print(titanic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### NOTES: titanic, pull data straight from the web\n",
    "'''\n",
    "This may result in an error: \"certificate verify failed: unable to get local issuer certificate \"\n",
    "This has something to do with the Python version Jupyter is using vs the native Python version on your machine.\n",
    "The code below should solve the issue OR students can run as a Python file without issues.\n",
    "\n",
    "Solution:\n",
    "```\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "```\n",
    "\n",
    "'''\n",
    "titanic_data = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
    "print(titanic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### NOTES: titanic\n",
    "\n",
    "# Rename Column Names:\n",
    "col_names = [\"Id\", \"Survived\", \n",
    "                \"Passenger Class\", \"Full Name\", \n",
    "                \"Gender\", \"Age\", \"SibSp\", \"Parch\", \n",
    "                \"Ticket Number\", \"Price\", \"Cabin\", \"Station\"] \n",
    "\n",
    "# NOTE about r-strings:\n",
    "# An r-string is a raw string.\n",
    "# It ignores escape characters. For example, \"\\n\" is a string containing a newline character, and r\"\\n\" is a string containing a backslash and the letter n.\n",
    "# We use them here to avoid escaping our string earlier when providing the file path\n",
    "\n",
    "# Will add the new columns above the original header\n",
    "titanic_data = pd.read_csv(r\"./resources/titanic.csv\", names = col_names)\n",
    "# print(titanic_data.head())\n",
    "\n",
    "# Skip Rows - expects an integer (number of rows at top of file to skip), or list of row numbers to skip individually\n",
    "# here, we are skipping the first row (0-based)\n",
    "titanic_data = pd.read_csv(r\"./resources/titanic.csv\", names = col_names, skiprows=[0])\n",
    "# print(titanic_data.head())\n",
    "\n",
    "# Save data to a new .csv file:\n",
    "titanic_data = pd.read_csv(r\"./resources/titanic.csv\", names = col_names, skiprows=[0])\n",
    "titanic_data.to_csv('use_titanic.csv', index=False)\n",
    "\n",
    "# # Viewing the newly created .csv file\n",
    "df = pd.read_csv('use_titanic.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
